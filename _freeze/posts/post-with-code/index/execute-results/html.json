{
  "hash": "4c6b290baa576fbd207339cde9658ced",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Robust SQL calls from Python\"\nauthor: \"Dekel Wainer\"\ndate: \"2025-05-29\"\ncategories: [SQL, Python, reliability]\nimage: \"image.png\"\n---\n\n\nMaking Python code interact with an SQL database is straightforward.\nImagine you're using a cloud SQL and want to insert data into 2 tables. You might generate this code:\n\n::: {#759460b3 .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport pyodbc\n\nconn_str = os.getenv(\"sql_connection_string\")  # secret connection string\n\nwith pyodbc.connect(conn_str) as conn:\n    with conn.cursor() as cursor:\n\n        insert_query = \"\"\"\n            INSERT INTO some_table (field1, field2, field3)  -- fieldN are the actual fields in the database\n            VALUES (?, ?, ?)\n        \"\"\"\n        cursor.execute(insert_query, (field1, field2, field3))  # fieldN here are the variables in Python\n        conn.commit()\n\n        insert_query = \"\"\"\n            INSERT INTO another_table (field4, field5)\n            VALUES (?, ?)\n        \"\"\"\n        cursor.execute(insert_query, (field4, field5)) \n        conn.commit()\n```\n:::\n\n\nA vibe-coding developer might stop here, assured by the LLM that this is good implementation because:\n    - The connection string isn't being exposed.\n    - Placeholders (the '?') are being used, which protects against SQL injection.\n    - A context manager is used (the 'with' statements), which automatically closes the connection to the SQL database.\n\nBut it's not good implementation, because there's no error handling.\nThis means any communication or schema error will result in unexpected behavior in the program.\nWorse still, if an error occurs after the first query is committed but the second isn't, your database only contains partial data.\n\nSo a better approach is to add explicit error handling, and commit data after all queries have succeeded.\nNothing gets committed unless everything runs smoothly.\n\n::: {#1f29d3c3 .cell execution_count=2}\n``` {.python .cell-code}\ntry:\n    with pyodbc.connect(conn_str) as conn:\n        with conn.cursor() as cursor:\n            insert_query_1 = \"\"\"\n                INSERT INTO some_table (field1, field2, field3)\n                VALUES (?, ?, ?)\n            \"\"\"\n            cursor.execute(insert_query_1, (field1, field2, field3))\n\n            insert_query_2 = \"\"\"\n                INSERT INTO another_table (field4, field5)\n                VALUES (?, ?)\n            \"\"\"\n            cursor.execute(insert_query_2, (field4, field5))\n\n        # commit once\n        conn.commit()\n\nexcept Exception as e:\n    print(f\"Database operation failed: {e}\")\n    conn.rollback()\n```\n:::\n\n\nBut what if you don't have all the information needed to commit concurrently in a single script?\nFor instance, if your program collects data across dozens of scripts - there is no opportunity to commit everything at once.\n\nInstead, you can call a stable class into each relevant script, and append the query you want to commit.\nThen, at some later point in the program, you loop over that list and write the queries to the database.\n\n::: {#a3c36317 .cell execution_count=3}\n``` {.python .cell-code}\n# define this class in a helper module\nclass QueryCollector:\n    def __init__(self):\n        self.queries = []\n\n    def add(self, query, params):\n        self.queries.append((query, params))\n\n    def execute_all(self, conn_str):\n        try:\n            with pyodbc.connect(conn_str) as conn:\n                with conn.cursor() as cursor:\n                    for query, params in self.queries:\n                        cursor.execute(query, params)\n                conn.commit()\n        except Exception as e:\n            print(f\"Error during batched SQL execution: {e}\")\n            conn.rollback()\n\n\n# call the class in any script that needs it\ncollector = QueryCollector()\n\ncollector.add(\n    \"INSERT INTO some_table (field1, field2, field3) VALUES (?, ?, ?)\",\n    (field1, field2, field3)\n)\n\n\n# commit everything at a later point in the program, once you collected all your queries\ncollector.execute_all(os.getenv(\"sql_connection_string\"))\n```\n:::\n\n\nNow you have a centralized solution which is easy to expand. For example, you can add complex error handling, query-level logging, retries, or group commits to reduce I/O.‚Äù\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}